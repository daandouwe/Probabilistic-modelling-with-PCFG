{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM estimation using Inside-Outside\n",
    "\n",
    "This notebook essentially builts forward on the notebook lab-inference.ipynb. (All code is mine unless explicitly noted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rule import Rule\n",
    "from cfg import WCFG, read_grammar_rules\n",
    "from parser import cky\n",
    "from earley import earley\n",
    "from symbol import make_symbol, is_nonterminal, is_terminal\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T] -> [P] (0.5)\n",
      "[T] -> [T] * [P] (0.4)\n",
      "[T] -> [T] + [P] (0.1)\n",
      "[E] -> [T] (0.5)\n",
      "[E] -> [E] + [T] (0.45)\n",
      "[E] -> [E] * [T] (0.05)\n",
      "[P] -> a (1.0)\n"
     ]
    }
   ],
   "source": [
    "# let's use the ambiguous grammar\n",
    "G = WCFG(read_grammar_rules(open('examples/ambiguous', 'r')))\n",
    "print G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', '+', 'a', '*', 'a']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'a + a * a'.split()\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E:2-3] -> [T:2-3] (0.5)\n",
      "[E:4-5] -> [T:4-5] (0.5)\n",
      "[T:0-3] -> [T:0-1] + [P:2-3] (0.1)\n",
      "[T:0-5] -> [T:0-3] * [P:4-5] (0.4)\n",
      "[E:0-5] -> [T:0-5] (0.5)\n",
      "[E:0-5] -> [E:0-3] * [T:4-5] (0.05)\n",
      "[E:0-5] -> [E:0-1] + [T:2-5] (0.45)\n",
      "[T:0-1] -> [P:0-1] (0.5)\n",
      "[E:0-1] -> [T:0-1] (0.5)\n",
      "[E:0-3] -> [E:0-1] + [T:2-3] (0.45)\n",
      "[E:0-3] -> [T:0-3] (0.5)\n",
      "[E:2-5] -> [E:2-3] * [T:4-5] (0.05)\n",
      "[E:2-5] -> [T:2-5] (0.5)\n",
      "[P:0-1] -> a (1.0)\n",
      "[T:4-5] -> [P:4-5] (0.5)\n",
      "[P:2-3] -> a (1.0)\n",
      "[T:2-5] -> [T:2-3] * [P:4-5] (0.4)\n",
      "[T:2-3] -> [P:2-3] (0.5)\n",
      "[P:4-5] -> a (1.0)\n"
     ]
    }
   ],
   "source": [
    "forest = cky(G, sentence)\n",
    "print forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: the goal symbol after parsing is the original *start* symbol annotated from *0* to *n* (the length of the sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[E:0-5]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal = make_symbol('[E]', 0, len(sentence))\n",
    "goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use earley to parse the sentence. For this we also need to pass the parser the start symbol that we are using, as earley uses this to produce its axioms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T:0-3] -> [T:0-1] + [P:2-3] (0.1)\n",
      "[T:0-5] -> [T:0-3] * [P:4-5] (0.4)\n",
      "[E:0-5] -> [T:0-5] (0.5)\n",
      "[E:0-5] -> [E:0-3] * [T:4-5] (0.05)\n",
      "[E:0-5] -> [E:0-1] + [T:2-5] (0.45)\n",
      "[T:0-1] -> [P:0-1] (0.5)\n",
      "[E:0-1] -> [T:0-1] (0.5)\n",
      "[E:0-3] -> [E:0-1] + [T:2-3] (0.45)\n",
      "[E:0-3] -> [T:0-3] (0.5)\n",
      "[P:0-1] -> a (1.0)\n",
      "[T:4-5] -> [P:4-5] (0.5)\n",
      "[P:2-3] -> a (1.0)\n",
      "[T:2-5] -> [T:2-3] * [P:4-5] (0.4)\n",
      "[T:2-3] -> [P:2-3] (0.5)\n",
      "[P:4-5] -> a (1.0)\n"
     ]
    }
   ],
   "source": [
    "earley_forest = earley(G, sentence, start='[E]')\n",
    "print earley_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the forest generated by Earley are smaller than the forest generated by CKY: Earley has a built-in top-down filter that removes redundant elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print len(forest)\n",
    "print len(earley_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a corpus\n",
    "\n",
    "We can use a grammar generate a corpus using ancestral sampling. Here we use the ambiguous grammar given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', '*', 'a', '+', 'a'], ['a'], ['a'], ['a', '*', 'a', '+', 'a'], ['a', '*', 'a', '+', 'a', '+', 'a'], ['a', '*', 'a', '*', 'a', '+', 'a', '+', 'a'], ['a', '*', 'a'], ['a', '+', 'a', '*', 'a', '*', 'a'], ['a'], ['a', '*', 'a', '+', 'a', '*', 'a', '+', 'a', '+', 'a'], ['a', '*', 'a', '*', 'a', '+', 'a', '+', 'a'], ['a', '+', 'a'], ['a', '*', 'a'], ['a'], ['a', '*', 'a', '+', 'a', '+', 'a', '*', 'a', '*', 'a', '*', 'a', '*', 'a', '*', 'a'], ['a'], ['a', '*', 'a', '+', 'a', '+', 'a', '+', 'a', '*', 'a', '+', 'a', '+', 'a', '*', 'a', '+', 'a', '*', 'a'], ['a', '+', 'a', '*', 'a', '*', 'a'], ['a'], ['a'], ['a', '+', 'a', '*', 'a'], ['a', '+', 'a', '+', 'a', '+', 'a', '+', 'a', '*', 'a', '*', 'a', '*', 'a', '+', 'a'], ['a'], ['a'], ['a', '+', 'a'], ['a', '*', 'a'], ['a'], ['a', '*', 'a', '+', 'a', '*', 'a', '*', 'a', '*', 'a', '+', 'a'], ['a', '*', 'a', '*', 'a'], ['a'], ['a', '*', 'a', '*', 'a', '*', 'a', '+', 'a', '+', 'a', '+', 'a', '*', 'a'], ['a', '*', 'a', '+', 'a', '+', 'a', '+', 'a', '*', 'a', '+', 'a', '+', 'a', '+', 'a'], ['a', '+', 'a'], ['a'], ['a', '+', 'a', '*', 'a'], ['a', '+', 'a', '*', 'a', '*', 'a', '+', 'a', '*', 'a', '*', 'a', '*', 'a', '+', 'a'], ['a'], ['a'], ['a'], ['a', '+', 'a', '*', 'a', '+', 'a', '*', 'a', '+', 'a', '+', 'a'], ['a'], ['a', '+', 'a', '+', 'a'], ['a', '+', 'a', '*', 'a'], ['a'], ['a', '+', 'a', '*', 'a'], ['a', '+', 'a', '*', 'a'], ['a', '*', 'a'], ['a', '+', 'a', '+', 'a'], ['a'], ['a', '*', 'a'], ['a', '*', 'a'], ['a', '+', 'a'], ['a', '*', 'a', '+', 'a', '+', 'a', '*', 'a'], ['a', '+', 'a', '+', 'a'], ['a', '+', 'a'], ['a', '*', 'a', '*', 'a', '*', 'a'], ['a', '+', 'a', '*', 'a', '*', 'a', '*', 'a', '+', 'a', '*', 'a', '+', 'a', '+', 'a', '*', 'a', '+', 'a'], ['a', '*', 'a', '*', 'a', '+', 'a', '+', 'a', '+', 'a', '*', 'a'], ['a', '+', 'a'], ['a', '*', 'a', '+', 'a'], ['a'], ['a'], ['a', '+', 'a'], ['a', '*', 'a', '*', 'a', '+', 'a', '*', 'a', '+', 'a', '+', 'a'], ['a', '*', 'a', '+', 'a', '*', 'a', '+', 'a', '*', 'a', '*', 'a', '*', 'a'], ['a'], ['a', '+', 'a', '+', 'a', '+', 'a'], ['a', '+', 'a', '*', 'a', '*', 'a', '*', 'a', '*', 'a', '*', 'a', '*', 'a', '+', 'a', '+', 'a', '*', 'a', '*', 'a'], ['a', '+', 'a', '*', 'a', '*', 'a', '*', 'a'], ['a', '+', 'a', '*', 'a', '+', 'a'], ['a', '*', 'a'], ['a', '+', 'a', '*', 'a', '+', 'a', '*', 'a', '*', 'a', '+', 'a', '+', 'a', '*', 'a', '+', 'a', '*', 'a', '*', 'a'], ['a', '*', 'a', '+', 'a', '*', 'a', '*', 'a', '*', 'a', '+', 'a'], ['a', '*', 'a', '+', 'a', '+', 'a', '+', 'a'], ['a', '*', 'a', '*', 'a', '+', 'a', '*', 'a', '+', 'a', '*', 'a', '+', 'a'], ['a', '*', 'a', '*', 'a', '+', 'a'], ['a', '+', 'a'], ['a', '*', 'a', '*', 'a'], ['a'], ['a', '*', 'a', '*', 'a', '*', 'a'], ['a'], ['a'], ['a'], ['a', '+', 'a', '*', 'a', '+', 'a', '+', 'a', '+', 'a', '*', 'a', '*', 'a'], ['a', '+', 'a', '*', 'a'], ['a', '*', 'a', '+', 'a', '*', 'a', '*', 'a', '*', 'a', '*', 'a'], ['a', '*', 'a', '+', 'a', '+', 'a'], ['a', '*', 'a', '+', 'a', '*', 'a', '+', 'a'], ['a', '+', 'a', '*', 'a', '*', 'a', '*', 'a', '*', 'a', '+', 'a', '*', 'a', '+', 'a', '+', 'a'], ['a'], ['a', '+', 'a', '+', 'a', '*', 'a', '*', 'a', '*', 'a', '+', 'a'], ['a', '*', 'a'], ['a', '*', 'a', '*', 'a', '+', 'a', '*', 'a', '*', 'a', '+', 'a', '+', 'a', '+', 'a', '*', 'a', '+', 'a', '*', 'a'], ['a'], ['a', '*', 'a'], ['a', '*', 'a', '+', 'a', '+', 'a', '*', 'a', '+', 'a', '*', 'a', '*', 'a', '*', 'a', '+', 'a'], ['a', '+', 'a'], ['a'], ['a'], ['a', '+', 'a']]\n"
     ]
    }
   ],
   "source": [
    "def generate_sample(grammar, items=('[E]',)):\n",
    "    \"\"\"\n",
    "    Given a grammar returns a sentence from it using\n",
    "    the probabilities specfied in the grammar.\n",
    "    :param items: call the function with (start,) where \n",
    "                  start is the start symbol of the grammar\n",
    "    :returns: a sentence from the language as a list\n",
    "    \"\"\"\n",
    "    frags = []\n",
    "    for item in items:\n",
    "        if is_nonterminal(item):\n",
    "            productions = grammar.get(item)\n",
    "            ps = [production.prob for production in productions]\n",
    "            random_index = np.argmax(np.random.multinomial(1, ps, size=1))\n",
    "            prod = productions[random_index]\n",
    "            frags.extend(generate_sample(grammar, items=prod.rhs))\n",
    "        else:\n",
    "            frags.append(item)\n",
    "    return frags\n",
    "\n",
    "# print generate_sample(G)\n",
    "\n",
    "def generate_corpus(grammar, n, start=('[E]',),):\n",
    "    \"\"\"\n",
    "    Generates a corpus using the grammar\n",
    "    :param n: size of the corpus\n",
    "    :params: same a s generate corpus\n",
    "    :returns: a corpus in the form of a list\n",
    "    \"\"\"\n",
    "    return [generate_sample(grammar, items=start) for i in range(n)]\n",
    "\n",
    "corpus1 = generate_corpus(G, 100)\n",
    "corpus2 = generate_corpus(G, 1000)\n",
    "corpus3 = generate_corpus(G, 10000)\n",
    "\n",
    "print corpus1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside weights\n",
    "\n",
    "(Author: Wilker Aziz)\n",
    "\n",
    "The inside recursion accumulates the weight of all subtrees under a certain node.\n",
    "\n",
    "        I(v) = \n",
    "            1                           if v is terminal\n",
    "            0                           if v is nonterminal and BS(v) is empty\n",
    "            \n",
    "$$\\sum_{e \\in BS(v)} w(e) \\prod_{u \\in tail(e)} I(u)$$\n",
    "                                        otherwise\n",
    "                                        \n",
    "Here we are going to compute inside weights for acyclic forests, for a more general treatment see Goodman's \"Semiring Parsing\" paper (1999).\n",
    "\n",
    "Inside weights can be used, for instance, to answer the question:\n",
    "\n",
    "* what is the probability of sentence x?\n",
    "\n",
    "It can also be used to find the best derivation and to sample derivations, as we will show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def inside(forest, start):  # acyclic hypergraph\n",
    "    \n",
    "    \"\"\"\n",
    "    Author: Wilker Aziz\n",
    "    \n",
    "    The inside recursion for acyclic hypergraphs.\n",
    "    \n",
    "    :param forest: an acyclic WCFG\n",
    "    :param start: the start symbol (str)\n",
    "    :returns: a dictionary mapping a symbol (terminal or noterminal) to its inside weight\n",
    "    \"\"\"\n",
    "    I = dict()\n",
    "    \n",
    "    def get_inside(symbol):\n",
    "        \"\"\"computes inside recursively\"\"\"\n",
    "        w = I.get(symbol, None)\n",
    "        if w is not None:  # already computed\n",
    "            return w\n",
    "        incoming = forest.get(symbol, set())\n",
    "        if len(incoming) == 0:  # terminals have already been handled, this must be a nonterminal dead end\n",
    "            # store it to avoid repeating computation in the future\n",
    "            I[symbol] = 0.0\n",
    "            return 0.0\n",
    "        # accumulate the inside contribution of each incoming edge\n",
    "        w = 0.0\n",
    "        for rule in incoming:\n",
    "            k = rule.prob\n",
    "            for child in rule.rhs:\n",
    "                k *= get_inside(child)\n",
    "            w += k\n",
    "        # store it to avoid repeating computation in the future\n",
    "        I[symbol] = w\n",
    "        return w\n",
    "    \n",
    "    # handles terminals\n",
    "    for sym in forest.terminals:\n",
    "        I[sym] = 1.0\n",
    "    # recursively solves the inside formula from the start symbol\n",
    "    get_inside(start)\n",
    "        \n",
    "    return I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inside at the root represents the probability of the sentence:\n",
    "$$p(x) = \\sum_d p(x, d)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1.0, '[T:0-3]': 0.05, '[T:0-5]': 0.020000000000000004, '[E:0-5]': 0.034531250000000006, '[T:0-1]': 0.5, '+': 1.0, '*': 1.0, '[E:0-3]': 0.08125, '[T:2-3]': 0.5, '[P:0-1]': 1.0, '[T:4-5]': 0.5, '[P:2-3]': 1.0, '[T:2-5]': 0.2, '[E:0-1]': 0.25, '[P:4-5]': 1.0}\n"
     ]
    }
   ],
   "source": [
    "I = inside(forest, goal)\n",
    "\n",
    "print I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outside weights\n",
    "\n",
    "Computing outside probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[E:0-5] -> [E:0-1] + [T:2-5] (0.45), [E:2-5] -> [T:2-5] (0.5)]\n"
     ]
    }
   ],
   "source": [
    "def get_rules_by_rhs(grammar, symbol):\n",
    "    rules = []\n",
    "    for rule in grammar:\n",
    "#         print rule\n",
    "        if symbol in rule.rhs:\n",
    "            rules.append(rule)\n",
    "    return rules\n",
    "\n",
    "print get_rules_by_rhs(forest, '[T:2-5]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outside(forest, start, inside_dict):\n",
    "    \n",
    "    I = dict()\n",
    "    \n",
    "    def get_outside(symbol):\n",
    "        w = I.get(symbol, None)\n",
    "        if w is not None:  # already computed\n",
    "            return w\n",
    "        outgoing = get_rules_by_rhs(forest, symbol)\n",
    "        beta = 0.0\n",
    "        for rule in outgoing:\n",
    "            k = rule.prob\n",
    "            for child in rule.rhs:\n",
    "                if child != symbol:\n",
    "                    try:\n",
    "                        alpha = inside_dict[child]\n",
    "                    except KeyError:\n",
    "                        # Not sure about this solution...\n",
    "                        # If child is not in inside_dict then child was not seen in the top-down process\n",
    "                        # of the inside algorithm, and hence there is no way to complete child into a \n",
    "                        # parse for the whole sentence. So beta should be 0.0\n",
    "#                         print \"key-error with {}\".format(child)\n",
    "                        alpha = 0.0\n",
    "                    k *= alpha\n",
    "            k *= get_outside(rule.lhs)\n",
    "            beta += k\n",
    "        I[symbol] = beta\n",
    "        return beta\n",
    "    \n",
    "    I[start] = 1.0\n",
    "    \n",
    "    for sym in forest.terminals:\n",
    "#         print \"terminal: {}\".format(sym)\n",
    "        I[sym] = get_outside(sym)\n",
    "    \n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0.10359375000000001, '[E:2-3]': 0.0, '[E:4-5]': 0.0, '[T:0-3]': 0.21250000000000002, '[T:0-5]': 0.5, '[E:0-5]': 1.0, '[T:0-1]': 0.06906250000000001, '[E:0-1]': 0.09562500000000002, '*': 0.03453125, '[E:0-3]': 0.025, '[T:4-5]': 0.0040625, '[P:0-1]': 0.034531250000000006, '[P:2-3]': 0.034531250000000006, '[E:2-5]': 0.0, '[T:2-5]': 0.1125, '+': 0.034531250000000006, '[T:2-3]': 0.04781250000000001, '[P:4-5]': 0.03453125}\n"
     ]
    }
   ],
   "source": [
    "print outside(forest, goal, I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside-Outside\n",
    "\n",
    "We use the inside-outside algorithm to learn probabilities of the rules from unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize(grammar, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Takes a grammar and returns that same grammar but\n",
    "    with the probabilities replaced by random probabilities\n",
    "    generated from a Dirichlet distribution.\n",
    "    :param: alpha is the Dirichlet concentration parameter\n",
    "    \"\"\"\n",
    "    init_grammar = WCFG()\n",
    "    for nonterminal in grammar.nonterminals:\n",
    "        rules = grammar.get(nonterminal)\n",
    "        init_prob = np.random.dirichlet(len(rules)*[alpha])\n",
    "        for i, rule in enumerate(rules):\n",
    "            init_grammar.add(Rule(rule.lhs, rule.rhs, init_prob[i]))\n",
    "    return init_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def get_instances(rule, forest):\n",
    "    \"\"\"\n",
    "    Given a rule\n",
    "    \n",
    "    A -> B C \n",
    "    \n",
    "    get_instances collects all instances of rules of the form\n",
    "    \n",
    "    [A:i-j] -> [B:i-k] [C:k-j] \n",
    "    \n",
    "    from the forest and returns them in a list.\n",
    "    \"\"\"\n",
    "    instances = []\n",
    "    for r in forest:\n",
    "        if r.lhs[1] == rule.lhs[1] and len(r.rhs)==len(rule.rhs):\n",
    "            test = []\n",
    "            for i in range(len(r.rhs)):\n",
    "                try:\n",
    "                    # if for example r.rhs[i] = '[A]' \n",
    "                    v = r.rhs[i][1] == rule.rhs[i][1]\n",
    "                    test.append(v)\n",
    "                except IndexError:\n",
    "                    # if for example r.rhs[i] = '*' \n",
    "                    v = r.rhs[i][0] == rule.rhs[i][0]\n",
    "                    test.append(v)\n",
    "            if np.all(test):\n",
    "                instances.append(r)\n",
    "    return instances\n",
    "\n",
    "def inside_outside(training_sents, grammar, start_sym='[E]'):\n",
    "    f = defaultdict(float)\n",
    "    for sent in training_sents:\n",
    "        forest = cky(grammar, sent)\n",
    "        goal = make_symbol(start_sym, 0, len(sent))\n",
    "        I = inside(forest, goal)\n",
    "        O = outside(forest, goal, I)     \n",
    "        for rule in grammar:\n",
    "            w = 0.0\n",
    "            for instance in get_instances(rule, forest):\n",
    "                k = instance.prob\n",
    "                k *= O[instance.lhs]\n",
    "                for child in instance.rhs:\n",
    "                    try:\n",
    "                        alpha = I[child]\n",
    "                    except KeyError:\n",
    "                        # same solution as in outside\n",
    "                        alpha = 0.0\n",
    "                    k *= alpha\n",
    "                w += k\n",
    "            f[rule] += w/I[goal]\n",
    "    return f\n",
    "\n",
    "def EM(training_sents, grammar, n, start_sym='[E]', prin=False):\n",
    "    if prin == True:\n",
    "        print \"Initalized grammar:\\n{}\\n\".format(grammar)\n",
    "    step = 0\n",
    "    while step < n:\n",
    "        \n",
    "        # E-step\n",
    "        f = inside_outside(training_sents, grammar, start_sym=start_sym)\n",
    "                \n",
    "        #M-step\n",
    "        new_grammar = WCFG()\n",
    "        for rule in grammar:\n",
    "            new_prob = f[rule]/sum([f[r] for r in grammar.get(rule.lhs)])\n",
    "            new_grammar.add(Rule(rule.lhs, rule.rhs, new_prob))\n",
    "        \n",
    "        grammar = new_grammar\n",
    "        \n",
    "        step +=1   \n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting progress\n",
    "\n",
    "We use pyplot to show the progress in the parameter estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_EM(corpus, grammar, n, start_sym='[E]', init=False):\n",
    "    \n",
    "    d = defaultdict(list)\n",
    "    for rule in grammar: # save the true grammar probs\n",
    "        ruled = Rule(rule.lhs, rule.rhs, 1.0) # make sure the dict entries do not depend on the rule probs\n",
    "        d[ruled].append(rule.prob)\n",
    "    \n",
    "    if init == True:\n",
    "        # initialize a random grammar based on grammar\n",
    "        grammar = initialize(grammar)\n",
    "    \n",
    "    i = 0\n",
    "    while i < n:\n",
    "        print \"round {}\".format(i)\n",
    "        new_grammar = EM(corpus, grammar, 1, start_sym=start_sym, prin=False)\n",
    "        for rule in new_grammar:\n",
    "            ruled = Rule(rule.lhs, rule.rhs, 1.0)\n",
    "            d[ruled].append(rule.prob)\n",
    "#             print \"rule {0}, prob {1}, rule-prob {2}\".format(rule, d[ruled], rule.prob)\n",
    "        \n",
    "#         grammar = WCFG()\n",
    "#         for rule in new_grammar:\n",
    "#             grammar.add(rule)\n",
    "        grammar = new_grammar\n",
    "        i += 1\n",
    "\n",
    "    colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', \n",
    "              'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "    i = 0\n",
    "    for rule, prob in d.iteritems():\n",
    "        plt.plot(range(n+1), prob, color=colors[i%len(colors)])\n",
    "        plt.plot(range(n+1), [prob[0]]*(n+1), '--', color=colors[i%len(colors)])\n",
    "        i += 1\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 0\n",
      "round 1\n",
      "round 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIIW7iIgDKdxFRBxI4S4i4kD/DwYPSNl9xvxNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1129f7910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_EM(corpus1, G, 3, init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is this not working?\n",
    "\n",
    "There seems to be a problem with the EM: only the first run changes the parameters; all later runs keep them identical. \n",
    "\n",
    "Below I investigate this and try to find the bug. I have not been succesfull yet however..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_grammar = initialize(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_grammar:\n",
      "[T] -> [P] (0.000473491343677)\n",
      "[T] -> [T] * [P] (0.553670923623)\n",
      "[T] -> [T] + [P] (0.445855585033)\n",
      "[E] -> [T] (0.559683910819)\n",
      "[E] -> [E] + [T] (0.187256900197)\n",
      "[E] -> [E] * [T] (0.253059188984)\n",
      "[P] -> a (1.0)\n",
      "\n",
      "\n",
      "f_init:\n",
      "[P] -> a (1.0) 408.0\n",
      "[E] -> [E] + [T] (0.187256900197) 0.0286306888768\n",
      "[E] -> [T] (0.559683910819) 100.0\n",
      "[T] -> [T] * [P] (0.553670923623) 163.964516021\n",
      "[T] -> [T] + [P] (0.445855585033) 143.971369311\n",
      "[T] -> [P] (0.000473491343677) 100.064114668\n",
      "[E] -> [E] * [T] (0.253059188984) 0.0354839787195\n",
      "\n",
      "\n",
      "rule: [T] -> [P] (0.000473491343677)\t f[rule]: 100.064114668\t sum: 408.0\t\n",
      "rule: [T] -> [T] * [P] (0.553670923623)\t f[rule]: 163.964516021\t sum: 408.0\t\n",
      "rule: [T] -> [T] + [P] (0.445855585033)\t f[rule]: 143.971369311\t sum: 408.0\t\n",
      "rule: [E] -> [T] (0.559683910819)\t f[rule]: 100.0\t sum: 100.064114668\t\n",
      "rule: [E] -> [E] + [T] (0.187256900197)\t f[rule]: 0.0286306888768\t sum: 100.064114668\t\n",
      "rule: [E] -> [E] * [T] (0.253059188984)\t f[rule]: 0.0354839787195\t sum: 100.064114668\t\n",
      "rule: [P] -> a (1.0)\t f[rule]: 408.0\t sum: 408.0\t\n",
      "\n",
      "new_grammar:\n",
      "[T] -> [P] (0.245255183009)\n",
      "[T] -> [T] * [P] (0.401873813778)\n",
      "[T] -> [T] + [P] (0.352871003214)\n",
      "[E] -> [T] (0.99935926413)\n",
      "[E] -> [E] + [T] (0.000286123441674)\n",
      "[E] -> [E] * [T] (0.000354612428615)\n",
      "[P] -> a (1.0)\n",
      "\n",
      "\n",
      "f_new:\n",
      "[E] -> [T] (0.99935926413) 100.0\n",
      "[P] -> a (1.0) 408.0\n",
      "[E] -> [E] + [T] (0.000286123441674) 0.0286306888768\n",
      "[E] -> [E] * [T] (0.000354612428615) 0.0354839787195\n",
      "[T] -> [T] + [P] (0.352871003214) 143.971369311\n",
      "[T] -> [T] * [P] (0.401873813778) 163.964516021\n",
      "[T] -> [P] (0.245255183009) 100.064114668\n",
      "\n",
      "\n",
      "rule: [T] -> [P] (0.245255183009)\t f[rule]: 100.064114668\t sum: 408.0\t\n",
      "rule: [T] -> [T] * [P] (0.401873813778)\t f[rule]: 163.964516021\t sum: 408.0\t\n",
      "rule: [T] -> [T] + [P] (0.352871003214)\t f[rule]: 143.971369311\t sum: 408.0\t\n",
      "rule: [E] -> [T] (0.99935926413)\t f[rule]: 100.0\t sum: 100.064114668\t\n",
      "rule: [E] -> [E] + [T] (0.000286123441674)\t f[rule]: 0.0286306888768\t sum: 100.064114668\t\n",
      "rule: [E] -> [E] * [T] (0.000354612428615)\t f[rule]: 0.0354839787195\t sum: 100.064114668\t\n",
      "rule: [P] -> a (1.0)\t f[rule]: 408.0\t sum: 408.0\t\n",
      "\n",
      "new_new_grammar:\n",
      "[T] -> [P] (0.245255183009)\n",
      "[T] -> [T] * [P] (0.401873813778)\n",
      "[T] -> [T] + [P] (0.352871003214)\n",
      "[E] -> [T] (0.99935926413)\n",
      "[E] -> [E] + [T] (0.000286123441674)\n",
      "[E] -> [E] * [T] (0.000354612428615)\n",
      "[P] -> a (1.0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"init_grammar:\"\n",
    "print init_grammar\n",
    "print '\\n'\n",
    "\n",
    "print 'f_init:'\n",
    "f = inside_outside(corpus1, init_grammar, start_sym='[E]')\n",
    "for rule, value in f.iteritems():\n",
    "    print rule, value\n",
    "print '\\n'\n",
    "\n",
    "new_grammar = WCFG()\n",
    "for rule in init_grammar:\n",
    "    print \"rule: {0}\\t f[rule]: {1}\\t sum: {2}\\t\".format(rule, \n",
    "                                                         f[rule], \n",
    "                                                         sum([f[r] for r in init_grammar.get(rule.lhs)]))\n",
    "    new_prob = f[rule]/sum([f[r] for r in init_grammar.get(rule.lhs)])\n",
    "    new_grammar.add(Rule(rule.lhs, rule.rhs, new_prob))\n",
    "print \"\\nnew_grammar:\"\n",
    "print new_grammar\n",
    "print '\\n'\n",
    "\n",
    "print 'f_new:'\n",
    "f_new = inside_outside(corpus1, new_grammar, start_sym='[E]')\n",
    "for rule, value in f_new.iteritems():\n",
    "    print rule, value\n",
    "print '\\n'\n",
    "\n",
    "newnew_grammar = WCFG()\n",
    "for rule in new_grammar:\n",
    "    print \"rule: {0}\\t f[rule]: {1}\\t sum: {2}\\t\".format(rule, \n",
    "                                                         f_new[rule], \n",
    "                                                         sum([f_new[r] for r in new_grammar.get(rule.lhs)]))\n",
    "    new_prob = f_new[rule]/sum([f_new[r] for r in new_grammar.get(rule.lhs)])\n",
    "    newnew_grammar.add(Rule(rule.lhs, rule.rhs, new_prob))\n",
    "\n",
    "print \"\\nnew_new_grammar:\"\n",
    "print newnew_grammar\n",
    "print '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on a natural language grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NN] -> mouse (0.33)\n",
      "[NN] -> cat (0.33)\n",
      "[NN] -> dog (0.34)\n",
      "[JJ] -> big (0.5)\n",
      "[JJ] -> black (0.5)\n",
      "[DT] -> the (0.5)\n",
      "[DT] -> a (0.5)\n",
      "[NPBAR] -> [JJ] [NN] (0.5)\n",
      "[NPBAR] -> [JJ] [NPBAR] (0.5)\n",
      "[VP] -> [VB] [NP] (1.0)\n",
      "[S] -> [NP] [VP] (1.0)\n",
      "[VB] -> chased (0.5)\n",
      "[VB] -> ate (0.5)\n",
      "[NP] -> [DT] [NN] (0.5)\n",
      "[NP] -> [DT] [NPBAR] (0.5)\n",
      "\n",
      "Some sentences generated by the grammar:\n",
      "\n",
      "['the', 'black', 'mouse', 'ate', 'a', 'black', 'big', 'dog']\n",
      "['the', 'big', 'black', 'cat', 'chased', 'a', 'dog']\n",
      "['the', 'dog', 'chased', 'a', 'black', 'mouse']\n"
     ]
    }
   ],
   "source": [
    "hdp_pcfg = WCFG(read_grammar_rules(open('examples/hdp-pcfg-grammar', 'r')))\n",
    "print hdp_pcfg\n",
    "hdp_corpus = generate_corpus(hdp_pcfg, 100, start=('[S]',))\n",
    "print '\\nSome sentences generated by the grammar:\\n'\n",
    "for i in range(3):\n",
    "    print hdp_corpus[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 0\n",
      "round 1\n",
      "round 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH37ZDA0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b8535d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_EM(hdp_corpus, hdp_pcfg, 3, start_sym='[S]', init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the NLTK toy-grammar\n",
    "\n",
    "Here we generate a corpus from a grammar taken from the NLTK toolkit. We use a Dirichlet distribution to give the productions random probabilities. Larger alpha gives more uniform distributions; smaller alpha gives more biased distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PP] -> [P] [NP] (1.0)\n",
      "[N] -> boy (0.0296057898203)\n",
      "[N] -> cookie (0.327153840769)\n",
      "[N] -> table (0.15643736648)\n",
      "[N] -> telescope (0.136339725333)\n",
      "[N] -> hill (0.350463277597)\n",
      "[VP] -> [V] [NP] (0.673929023594)\n",
      "[VP] -> [V] (0.110085414778)\n",
      "[VP] -> [VP] [PP] (0.215985561629)\n",
      "[V] -> saw (0.736539580502)\n",
      "[V] -> ate (0.230243141692)\n",
      "[V] -> ran (0.0332172778065)\n",
      "[Name] -> Bob (0.19425294971)\n",
      "[Name] -> Jack (0.80574705029)\n",
      "[Det] -> the (0.978510152527)\n",
      "[Det] -> a (0.0204459747726)\n",
      "[Det] -> my (0.00104387270073)\n",
      "[S] -> [NP] [VP] (1.0)\n",
      "[P] -> with (0.863229071555)\n",
      "[P] -> under (0.136770928445)\n",
      "[NP] -> [Det] [N] (0.772589057417)\n",
      "[NP] -> [Name] (0.225635714322)\n",
      "[NP] -> [NP] [PP] (0.00177522826084)\n"
     ]
    }
   ],
   "source": [
    "toy_grammar = WCFG(read_grammar_rules(open('examples/nltk-grammar', 'r')))\n",
    "# print toy_grammar\n",
    "\n",
    "# We give the grammar some random probabilities \n",
    "# that are not too uniform and moderately biased:\n",
    "toy_grammar = initialize(toy_grammar, alpha=0.5)\n",
    "print toy_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this grammar we can also generate a corpus. Note that the sentences can have an enormous number of different parses. The grammar can quite easliy generate very long sentences, and is very ambiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def counting(forest, start):  # acyclic hypergraph\n",
    "    \"\"\"\n",
    "    Author: Wilker Aziz\n",
    "    Taken from lab-inference.ipynb\n",
    "    \"\"\"\n",
    "    N = dict()\n",
    "    \n",
    "    def get_count(symbol):\n",
    "        w = N.get(symbol, None)\n",
    "        if w is not None:\n",
    "            return w\n",
    "        incoming = forest.get(symbol, set())\n",
    "        if len(incoming) == 0:  # terminals have already been handled, this must be a nonterminal dead end\n",
    "            N[symbol] = w\n",
    "            return 0\n",
    "        w = 0\n",
    "        for rule in incoming:\n",
    "            k = 1\n",
    "            for child in rule.rhs:\n",
    "                k *= get_count(child)\n",
    "            w += k\n",
    "        N[symbol] = w\n",
    "        return w\n",
    "    \n",
    "    # handles terminals\n",
    "    for sym in forest.terminals:\n",
    "        N[sym] = 1\n",
    "    # handles nonterminals\n",
    "    #for sym in forest.nonterminals:\n",
    "    #    get_inside(sym)\n",
    "    get_count(start)\n",
    "        \n",
    "    return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'hill', 'saw', 'the', 'telescope'], ['the', 'hill', 'saw', 'Jack', 'with', 'the', 'hill'], ['the', 'cookie', 'saw', 'Jack'], ['the', 'table', 'ate', 'the', 'telescope'], ['the', 'cookie', 'saw', 'the', 'telescope', 'with', 'the', 'cookie', 'with', 'the', 'hill'], ['the', 'hill', 'saw', 'the', 'hill'], ['Bob', 'ate', 'with', 'the', 'cookie'], ['the', 'cookie', 'saw', 'Jack'], ['the', 'cookie', 'ate', 'the', 'hill'], ['the', 'cookie', 'ate']]\n",
      "[1, 2, 1, 1, 5, 1, 1, 1, 1, 1, 5, 1, 1, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "generate_sample(toy_grammar, items=('[S]',))\n",
    "toy_corpus = generate_corpus(toy_grammar, 100, start=('[S]',))\n",
    "print toy_corpus[0:10]\n",
    "\n",
    "def checking_number_parses(n=100):\n",
    "    number = list()\n",
    "    for sentence in toy_corpus[0:n]:\n",
    "        toy_forest = cky(toy_grammar, sentence)\n",
    "        toy_goal = make_symbol('[S]', 0, len(sentence))\n",
    "        N_toy = counting(toy_forest, toy_goal)\n",
    "        number.append(N_toy[toy_goal])\n",
    "    return number\n",
    "\n",
    "print checking_number_parses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initalized grammar:\n",
      "[PP] -> [P] [NP] (1.0)\n",
      "[N] -> boy (0.40766118212)\n",
      "[N] -> cookie (0.0113249270264)\n",
      "[N] -> table (0.221688604954)\n",
      "[N] -> telescope (0.155110950027)\n",
      "[N] -> hill (0.204214335873)\n",
      "[Det] -> the (0.284674437759)\n",
      "[Det] -> a (0.71403694737)\n",
      "[Det] -> my (0.00128861487122)\n",
      "[V] -> saw (0.764847690343)\n",
      "[V] -> ate (0.00205289484053)\n",
      "[V] -> ran (0.233099414817)\n",
      "[Name] -> Bob (0.997424823817)\n",
      "[Name] -> Jack (0.00257517618278)\n",
      "[VP] -> [V] [NP] (0.792730416705)\n",
      "[VP] -> [V] (0.00443655114493)\n",
      "[VP] -> [VP] [PP] (0.20283303215)\n",
      "[S] -> [NP] [VP] (1.0)\n",
      "[P] -> with (0.290967841163)\n",
      "[P] -> under (0.709032158837)\n",
      "[NP] -> [Det] [N] (0.340651181316)\n",
      "[NP] -> [Name] (0.549476647475)\n",
      "[NP] -> [NP] [PP] (0.109872171209)\n",
      "\n",
      "\n",
      "Approximated grammar:\n",
      "[PP] -> [P] [NP] (1.0)\n",
      "[N] -> boy (0.464286640062)\n",
      "[N] -> cookie (0.0128979960532)\n",
      "[N] -> table (0.504168974444)\n",
      "[N] -> telescope (0.00450085262363)\n",
      "[N] -> hill (0.0141455368171)\n",
      "[Det] -> the (0.960893854749)\n",
      "[Det] -> a (0.0391061452514)\n",
      "[Det] -> my (0.0)\n",
      "[V] -> saw (0.618219530119)\n",
      "[V] -> ate (0.193368302964)\n",
      "[V] -> ran (0.188412166917)\n",
      "[Name] -> Bob (0.994870920509)\n",
      "[Name] -> Jack (0.00512907949062)\n",
      "[VP] -> [V] [NP] (0.693803696059)\n",
      "[VP] -> [V] (0.142104371482)\n",
      "[VP] -> [VP] [PP] (0.164091932459)\n",
      "[S] -> [NP] [VP] (1.0)\n",
      "[P] -> with (0.896551724138)\n",
      "[P] -> under (0.103448275862)\n",
      "[NP] -> [Det] [N] (0.808602373837)\n",
      "[NP] -> [Name] (0.149071946015)\n",
      "[NP] -> [NP] [PP] (0.0423256801486)\n",
      "\n",
      "Original grammar:\n",
      "[PP] -> [P] [NP] (1.0)\n",
      "[N] -> boy (0.0296057898203)\n",
      "[N] -> cookie (0.327153840769)\n",
      "[N] -> table (0.15643736648)\n",
      "[N] -> telescope (0.136339725333)\n",
      "[N] -> hill (0.350463277597)\n",
      "[VP] -> [V] [NP] (0.673929023594)\n",
      "[VP] -> [V] (0.110085414778)\n",
      "[VP] -> [VP] [PP] (0.215985561629)\n",
      "[V] -> saw (0.736539580502)\n",
      "[V] -> ate (0.230243141692)\n",
      "[V] -> ran (0.0332172778065)\n",
      "[Name] -> Bob (0.19425294971)\n",
      "[Name] -> Jack (0.80574705029)\n",
      "[Det] -> the (0.978510152527)\n",
      "[Det] -> a (0.0204459747726)\n",
      "[Det] -> my (0.00104387270073)\n",
      "[S] -> [NP] [VP] (1.0)\n",
      "[P] -> with (0.863229071555)\n",
      "[P] -> under (0.136770928445)\n",
      "[NP] -> [Det] [N] (0.772589057417)\n",
      "[NP] -> [Name] (0.225635714322)\n",
      "[NP] -> [NP] [PP] (0.00177522826084)\n",
      "\n",
      "Difference grammar:\n",
      "[PP] -> [P] [NP] (1.11022302463e-16)\n",
      "[N] -> boy (0.434680850242)\n",
      "[N] -> cookie (0.314255844716)\n",
      "[N] -> table (0.347731607963)\n",
      "[N] -> telescope (0.13183887271)\n",
      "[N] -> hill (0.33631774078)\n",
      "[VP] -> [V] [NP] (0.0198746724651)\n",
      "[VP] -> [V] (0.0320189567042)\n",
      "[VP] -> [VP] [PP] (0.0518936291693)\n",
      "[V] -> saw (0.118320050382)\n",
      "[V] -> ate (0.0368748387282)\n",
      "[V] -> ran (0.155194889111)\n",
      "[Name] -> Bob (0.8006179708)\n",
      "[Name] -> Jack (0.8006179708)\n",
      "[Det] -> the (0.0176162977781)\n",
      "[Det] -> a (0.0186601704788)\n",
      "[Det] -> my (0.00104387270073)\n",
      "[S] -> [NP] [VP] (0.0)\n",
      "[P] -> with (0.0333226525826)\n",
      "[P] -> under (0.0333226525826)\n",
      "[NP] -> [Det] [N] (0.0360133164196)\n",
      "[NP] -> [Name] (0.0765637683073)\n",
      "[NP] -> [NP] [PP] (0.0405504518877)\n"
     ]
    }
   ],
   "source": [
    "init_toy_grammar = initialize(toy_grammar)\n",
    "# print init_toy_grammar\n",
    "\n",
    "approx_grammar = EM(toy_corpus, init_toy_grammar, 1, start_sym='[S]', prin=True)\n",
    "print '\\nApproximated grammar:'\n",
    "print approx_grammar\n",
    "print '\\nOriginal grammar:'\n",
    "print toy_grammar\n",
    "\n",
    "def difference_grammar(one, another):\n",
    "    diff_grammar = WCFG()\n",
    "    for rule in one:\n",
    "        for r in another:\n",
    "            if r.rhs == rule.rhs and r.lhs == rule.lhs:\n",
    "                approx_prob = r.prob\n",
    "                break\n",
    "        diff_grammar.add(Rule(rule.lhs, rule.rhs, abs(rule.prob-approx_prob)))\n",
    "    return diff_grammar\n",
    "\n",
    "print '\\nDifference grammar:'\n",
    "print difference_grammar(toy_grammar, approx_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
